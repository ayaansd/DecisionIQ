# -------------------------------------------------
#  agents/llm_utils.py   (offline‚Äëonly version)
# -------------------------------------------------
import os
import torch
from pathlib import Path

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
)

# -----------------------------------------------------------------
# 1Ô∏è‚É£  OFFLINE / TELEMETRY SETTINGS  (set once, globally)
# -----------------------------------------------------------------
# These flags make ü§ó‚ÄëTransformers read only from the local cache
# and suppress the tiny usage‚Äëtelemetry ping.
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"
os.environ["HF_DATASETS_OFFLINE"] = "1"

# -----------------------------------------------------------------
# 2Ô∏è‚É£  CONFIGURATION ‚Äì pick the model you have stored locally
# -----------------------------------------------------------------
# Example paths ‚Äì change them to the folder where you cached the model.
# You can use any of the 7‚ÄëB models listed in the table in the answer
# (Mistral‚Äë7B, Falcon‚Äë7B, Llama‚Äë2‚Äë7B‚ÄëChat, Phi‚Äë2, Gemma‚Äë7B, ‚Ä¶).
LOCAL_MODEL_DIR = Path(
    os.getenv(
        "LOCAL_MODEL_DIR",
        # default to a folder inside your home directory
        "~/local-llms/mistral-7b"
    )
).expanduser()

# -----------------------------------------------------------------
# 3Ô∏è‚É£  QUANTISATION SETTINGS (4‚Äëbit is the sweet spot for 16‚ÄØGB GPUs)
# -----------------------------------------------------------------
quant_cfg = BitsAndBytesConfig(
    load_in_4bit=True,                     # switch to False for 8‚Äëbit or full FP16
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",            # best quality for LLMs
)

# -----------------------------------------------------------------
# 4Ô∏è‚É£  LOAD TOKENIZER & MODEL (once, at import time)
# -----------------------------------------------------------------
try:
    tokenizer = AutoTokenizer.from_pretrained(
        LOCAL_MODEL_DIR,
        trust_remote_code=False,
    )
    model = AutoModelForCausalLM.from_pretrained(
        LOCAL_MODEL_DIR,
        device_map="auto",                 # puts layers on GPU if available, else CPU
        quantization_config=quant_cfg,
        trust_remote_code=False,
    )
    _LOCAL_MODEL_READY = True
    print("‚úÖ Offline LLM loaded from:", LOCAL_MODEL_DIR)
except Exception as exc:                     # noqa: BLE001
    _LOCAL_MODEL_READY = False
    model = tokenizer = None
    print(f"‚ö†Ô∏è Could not load local model ({LOCAL_MODEL_DIR}): {exc}")

# -----------------------------------------------------------------
# 5Ô∏è‚É£  Helper that actually runs generation
# -----------------------------------------------------------------
def _generate_local(prompt: str, max_new: int = 200, temperature: float = 0.7) -> str:
    """
    Run the locally‚Äëloaded model (4‚Äëbit quantised) on *prompt*.
    Returns the generated text (no leading prompt echo).
    """
    if not _LOCAL_MODEL_READY:
        return "‚ùå Local model is not available."

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=1024,
    )
    # The pipeline will automatically move the tensors to the right device.
    output_ids = model.generate(
        **inputs,
        max_new_tokens=max_new,
        temperature=temperature,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
    )
    return tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()

# -----------------------------------------------------------------
# 6Ô∏è‚É£  PUBLIC API ‚Äì ONLY LOCAL, NO CLOUD
# -----------------------------------------------------------------
def call_llm_model(prompt: str, model_name: str = "local") -> str:
    """
    **Offline‚Äëonly** wrapper.

    Parameters
    ----------
    prompt : str
        The user message you want the LLM to answer.
    model_name : str, optional
        Only the literal string ``"local"`` is recognised.
        Any other value will raise an error ‚Äì this function never makes
        an HTTP request.

    Returns
    -------
    str
        The model‚Äôs response (or an error message if the model could
        not be loaded).
    """
    if model_name != "local":
        raise ValueError(
            "Only the local model is available in this offline build. "
            "Pass model_name='local'."
        )
    return _generate_local(prompt)


# -----------------------------------------------------------------
# 7Ô∏è‚É£  Example usage (run this file directly to test)
# -----------------------------------------------------------------
if __name__ == "__main__":
    test_prompt = (
        "Summarise the key insights from a quarterly sales table in two sentences."
    )
    print("\n--- Prompt ---------------------------------------------------")
    print(test_prompt)
    print("\n--- Model output --------------------------------------------")
    print(call_llm_model(test_prompt, model_name="local"))